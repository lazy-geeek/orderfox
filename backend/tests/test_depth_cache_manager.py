#!/usr/bin/env python3
"""
Comprehensive tests for DepthCacheManager implementation.
Tests multiple concurrent symbols, consistency, reconnection, and performance.
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Any, Set, Optional
import os
import sys
import statistics
import logging

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.config import settings
from app.core.logging_config import setup_logging, get_logger
from app.services.depth_cache_service import depth_cache_service
from app.api.v1.endpoints.connection_manager import ConnectionManager

# Setup logging
setup_logging("INFO")
logger = get_logger("test_depth_cache")


class DepthCacheTestCollector:
    """Collects data during tests for analysis."""
    
    def __init__(self):
        self.updates: Dict[str, List[Dict]] = {}
        self.timestamps: Dict[str, List[float]] = {}
        self.errors: List[Dict] = []
        self.connection_events: List[Dict] = []
        self.bandwidth_data: Dict[str, int] = {}
        
    def record_update(self, symbol: str, data: Dict[str, Any]):
        """Record an orderbook update."""
        if symbol not in self.updates:
            self.updates[symbol] = []
            self.timestamps[symbol] = []
            self.bandwidth_data[symbol] = 0
            
        self.updates[symbol].append(data)
        self.timestamps[symbol].append(time.time())
        
        # Estimate bandwidth (rough JSON size)
        self.bandwidth_data[symbol] += len(json.dumps(data))
        
    def record_error(self, symbol: str, error: str):
        """Record an error event."""
        self.errors.append({
            "symbol": symbol,
            "error": error,
            "timestamp": datetime.utcnow().isoformat()
        })
        
    def record_connection_event(self, symbol: str, event: str):
        """Record connection event."""
        self.connection_events.append({
            "symbol": symbol,
            "event": event,
            "timestamp": datetime.utcnow().isoformat()
        })
        
    def get_update_rate(self, symbol: str) -> float:
        """Calculate average updates per second for a symbol."""
        if symbol not in self.timestamps or len(self.timestamps[symbol]) < 2:
            return 0.0
            
        time_diff = self.timestamps[symbol][-1] - self.timestamps[symbol][0]
        if time_diff == 0:
            return 0.0
            
        return len(self.timestamps[symbol]) / time_diff
        
    def get_bandwidth_usage(self, symbol: str) -> Dict[str, float]:
        """Get bandwidth usage statistics."""
        if symbol not in self.bandwidth_data:
            return {"total_bytes": 0, "bytes_per_second": 0}
            
        total_bytes = self.bandwidth_data[symbol]
        
        if symbol in self.timestamps and len(self.timestamps[symbol]) >= 2:
            time_diff = self.timestamps[symbol][-1] - self.timestamps[symbol][0]
            bytes_per_second = total_bytes / time_diff if time_diff > 0 else 0
        else:
            bytes_per_second = 0
            
        return {
            "total_bytes": total_bytes,
            "bytes_per_second": bytes_per_second,
            "total_kb": total_bytes / 1024,
            "kb_per_second": bytes_per_second / 1024
        }


async def test_multiple_concurrent_symbols():
    """Test DepthCacheManager with multiple concurrent symbols."""
    test_symbols = ["BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT", "ADAUSDT"]
    collector = DepthCacheTestCollector()
    received_updates: Dict[str, int] = {symbol: 0 for symbol in test_symbols}
    
    async def update_callback(symbol: str, data: Dict[str, Any]):
        """Callback for depth cache updates."""
        received_updates[symbol] += 1
        collector.record_update(symbol, data)
        
        # Validate data structure
        assert "bids" in data
        assert "asks" in data
        assert "timestamp" in data
        assert len(data["bids"]) > 0
        assert len(data["asks"]) > 0
    
    try:
        # Initialize service
        await depth_cache_service.initialize()
        
        # Start depth caches for all symbols
        logger.info(f"Starting depth caches for {len(test_symbols)} symbols")
        start_time = time.time()
        
        for symbol in test_symbols:
            collector.record_connection_event(symbol, "starting")
            await depth_cache_service.start_depth_cache(symbol, update_callback)
            await asyncio.sleep(0.1)  # Small delay to avoid rate limits
        
        # Let it run for 30 seconds
        logger.info("Collecting data for 30 seconds...")
        await asyncio.sleep(30)
        
        # Analyze results
        print("\n=== Multiple Concurrent Symbols Test Results ===")
        
        all_symbols_received = True
        for symbol in test_symbols:
            updates = received_updates[symbol]
            update_rate = collector.get_update_rate(symbol)
            bandwidth = collector.get_bandwidth_usage(symbol)
            
            print(f"\n{symbol}:")
            print(f"  - Updates received: {updates}")
            print(f"  - Update rate: {update_rate:.2f} updates/second")
            print(f"  - Bandwidth: {bandwidth['kb_per_second']:.2f} KB/s")
            print(f"  - Total data: {bandwidth['total_kb']:.2f} KB")
            
            if updates == 0:
                all_symbols_received = False
                logger.error(f"  ‚ö†Ô∏è No updates received for {symbol}")
                
        assert all_symbols_received, "Not all symbols received updates"
        
        # Check for errors
        if collector.errors:
            logger.warning(f"\nErrors encountered: {len(collector.errors)}")
            for error in collector.errors[:5]:  # Show first 5 errors
                logger.warning(f"  - {error['symbol']}: {error['error']}")
        
        # Verify all symbols are active
        active_symbols = depth_cache_service.get_active_symbols()
        assert len(active_symbols) == len(test_symbols)
        
        print("\n‚úÖ Multiple concurrent symbols test passed!")
        
    finally:
        # Cleanup
        for symbol in test_symbols:
            await depth_cache_service.stop_depth_cache(symbol)
        await depth_cache_service.shutdown()


async def test_orderbook_consistency():
    """Verify order book consistency between DepthCacheManager and current implementation."""
    symbol = "BTCUSDT"
    comparison_duration = 20  # seconds
    
    depth_cache_data = []
    ccxt_data = []
    
    async def depth_cache_callback(sym: str, data: Dict[str, Any]):
        """Collect depth cache data."""
        depth_cache_data.append({
            "timestamp": time.time(),
            "best_bid": float(data["bids"][0][0]) if data["bids"] else 0,
            "best_ask": float(data["asks"][0][0]) if data["asks"] else 0,
            "bid_count": len(data["bids"]),
            "ask_count": len(data["asks"])
        })
    
    try:
        # Start DepthCacheManager
        await depth_cache_service.initialize()
        await depth_cache_service.start_depth_cache(symbol, depth_cache_callback)
        
        # Start ConnectionManager (current implementation)
        connection_manager = ConnectionManager()
        
        # Create a mock connection to collect ccxt data
        connection_id = "test_consistency"
        
        async def ccxt_callback(data: Dict[str, Any]):
            """Collect ccxt data."""
            if data.get("type") == "orderbook" and data.get("data"):
                orderbook = data["data"]
                ccxt_data.append({
                    "timestamp": time.time(),
                    "best_bid": orderbook["bids"][0]["price"] if orderbook["bids"] else 0,
                    "best_ask": orderbook["asks"][0]["price"] if orderbook["asks"] else 0,
                    "bid_count": len(orderbook["bids"]),
                    "ask_count": len(orderbook["asks"])
                })
        
        # Subscribe to orderbook updates
        await connection_manager.connect()
        await connection_manager.subscribe_to_orderbook(
            connection_id, 
            symbol, 
            ccxt_callback,
            limit=1000  # Get more levels for comparison
        )
        
        # Collect data
        logger.info(f"Collecting orderbook data for {comparison_duration} seconds...")
        await asyncio.sleep(comparison_duration)
        
        # Analyze consistency
        print("\n=== Order Book Consistency Test Results ===")
        
        print(f"DepthCacheManager updates: {len(depth_cache_data)}")
        print(f"CCXT updates: {len(ccxt_data)}")
        
        if depth_cache_data and ccxt_data:
            # Compare best bid/ask prices
            price_diffs = []
            
            # Find overlapping time periods
            for dc_point in depth_cache_data[-10:]:  # Last 10 updates
                dc_time = dc_point["timestamp"]
                
                # Find closest ccxt update
                closest_ccxt = min(ccxt_data, key=lambda x: abs(x["timestamp"] - dc_time))
                
                if abs(closest_ccxt["timestamp"] - dc_time) < 1.0:  # Within 1 second
                    bid_diff = abs(dc_point["best_bid"] - closest_ccxt["best_bid"])
                    ask_diff = abs(dc_point["best_ask"] - closest_ccxt["best_ask"])
                    
                    price_diffs.append({
                        "bid_diff": bid_diff,
                        "ask_diff": ask_diff,
                        "dc_levels": (dc_point["bid_count"], dc_point["ask_count"]),
                        "ccxt_levels": (closest_ccxt["bid_count"], closest_ccxt["ask_count"])
                    })
            
            if price_diffs:
                avg_bid_diff = statistics.mean([d["bid_diff"] for d in price_diffs])
                avg_ask_diff = statistics.mean([d["ask_diff"] for d in price_diffs])
                
                print(f"\nPrice consistency analysis (last {len(price_diffs)} comparable updates):")
                print(f"  - Average bid price difference: ${avg_bid_diff:.4f}")
                print(f"  - Average ask price difference: ${avg_ask_diff:.4f}")
                
                # Check depth levels
                dc_avg_bids = statistics.mean([d["dc_levels"][0] for d in price_diffs])
                dc_avg_asks = statistics.mean([d["dc_levels"][1] for d in price_diffs])
                ccxt_avg_bids = statistics.mean([d["ccxt_levels"][0] for d in price_diffs])
                ccxt_avg_asks = statistics.mean([d["ccxt_levels"][1] for d in price_diffs])
                
                print(f"\nDepth comparison:")
                print(f"  - DepthCacheManager avg levels: {dc_avg_bids:.0f} bids, {dc_avg_asks:.0f} asks")
                print(f"  - CCXT avg levels: {ccxt_avg_bids:.0f} bids, {ccxt_avg_asks:.0f} asks")
                
                # Assert reasonable consistency (prices should match within $1)
                assert avg_bid_diff < 1.0, f"Bid prices differ too much: ${avg_bid_diff:.4f}"
                assert avg_ask_diff < 1.0, f"Ask prices differ too much: ${avg_ask_diff:.4f}"
                
                # DepthCacheManager should provide more levels
                assert dc_avg_bids > ccxt_avg_bids * 0.5, "DepthCacheManager has significantly fewer bid levels"
                assert dc_avg_asks > ccxt_avg_asks * 0.5, "DepthCacheManager has significantly fewer ask levels"
                
                print("\n‚úÖ Order book consistency test passed!")
            else:
                logger.warning("Could not find overlapping updates for comparison")
                
    finally:
        # Cleanup
        await depth_cache_service.stop_depth_cache(symbol)
        await connection_manager.unsubscribe_from_orderbook(connection_id, symbol)
        await connection_manager.disconnect()
        await depth_cache_service.shutdown()


async def test_automatic_reconnection():
    """Test automatic reconnection and error recovery."""
    symbol = "BTCUSDT"
    updates_before_disconnect = []
    updates_after_reconnect = []
    disconnect_detected = False
    reconnect_detected = False
    
    async def update_callback(sym: str, data: Dict[str, Any]):
        """Track updates and detect reconnection."""
        nonlocal disconnect_detected, reconnect_detected
        
        timestamp = time.time()
        
        if not disconnect_detected:
            updates_before_disconnect.append(timestamp)
        else:
            if not reconnect_detected and len(updates_after_reconnect) == 0:
                reconnect_detected = True
                logger.info(f"Reconnection detected at {datetime.utcnow().isoformat()}")
            updates_after_reconnect.append(timestamp)
    
    try:
        # Initialize and start
        await depth_cache_service.initialize()
        await depth_cache_service.start_depth_cache(symbol, update_callback)
        
        # Collect updates for 10 seconds
        logger.info("Collecting initial updates for 10 seconds...")
        await asyncio.sleep(10)
        
        # Simulate disconnection by manipulating internal state
        logger.info("Simulating disconnection...")
        if symbol in depth_cache_service.active_streams:
            task = depth_cache_service.active_streams[symbol]
            task.cancel()
            disconnect_detected = True
            logger.info("Disconnection simulated")
        
        # Wait for automatic reconnection
        logger.info("Waiting for automatic reconnection (up to 30 seconds)...")
        max_wait = 30
        start_wait = time.time()
        
        while len(updates_after_reconnect) < 5 and (time.time() - start_wait) < max_wait:
            await asyncio.sleep(1)
        
        # Analyze results
        print("\n=== Automatic Reconnection Test Results ===")
        print(f"Updates before disconnect: {len(updates_before_disconnect)}")
        print(f"Updates after reconnect: {len(updates_after_reconnect)}")
        
        if updates_before_disconnect and updates_after_reconnect:
            # Calculate downtime
            last_before = updates_before_disconnect[-1]
            first_after = updates_after_reconnect[0]
            downtime = first_after - last_before
            
            print(f"Downtime: {downtime:.2f} seconds")
            print(f"Reconnection successful: {reconnect_detected}")
            
            # Assert reconnection happened within reasonable time
            assert downtime < 60, f"Reconnection took too long: {downtime:.2f} seconds"
            assert reconnect_detected, "Reconnection was not detected"
            assert len(updates_after_reconnect) >= 5, "Not enough updates after reconnection"
            
            print("\n‚úÖ Automatic reconnection test passed!")
        else:
            print("Failed to collect sufficient data for reconnection test")
            
    finally:
        # Cleanup
        await depth_cache_service.stop_depth_cache(symbol)
        await depth_cache_service.shutdown()


async def test_performance_comparison():
    """Performance comparison between DepthCacheManager and current implementation."""
    symbol = "BTCUSDT"
    test_duration = 30
    
    # Metrics for DepthCacheManager
    dc_metrics = {
        "updates": 0,
        "processing_times": [],
        "bandwidth": 0,
        "start_time": 0,
        "end_time": 0
    }
    
    # Metrics for CCXT
    ccxt_metrics = {
        "updates": 0,
        "processing_times": [],
        "bandwidth": 0,
        "start_time": 0,
        "end_time": 0
    }
    
    async def dc_callback(sym: str, data: Dict[str, Any]):
        """DepthCacheManager callback with timing."""
        start = time.time()
        dc_metrics["updates"] += 1
        dc_metrics["bandwidth"] += len(json.dumps(data))
        
        if dc_metrics["start_time"] == 0:
            dc_metrics["start_time"] = start
        dc_metrics["end_time"] = start
        
        # Simulate processing
        _ = len(data["bids"]) + len(data["asks"])
        
        dc_metrics["processing_times"].append(time.time() - start)
    
    try:
        # Test DepthCacheManager
        logger.info("Testing DepthCacheManager performance...")
        await depth_cache_service.initialize()
        await depth_cache_service.start_depth_cache(symbol, dc_callback)
        await asyncio.sleep(test_duration)
        await depth_cache_service.stop_depth_cache(symbol)
        
        # Test CCXT implementation
        logger.info("Testing CCXT implementation performance...")
        connection_manager = ConnectionManager()
        await connection_manager.connect()
        
        connection_id = "perf_test"
        
        async def ccxt_callback(data: Dict[str, Any]):
            """CCXT callback with timing."""
            if data.get("type") == "orderbook":
                start = time.time()
                ccxt_metrics["updates"] += 1
                ccxt_metrics["bandwidth"] += len(json.dumps(data))
                
                if ccxt_metrics["start_time"] == 0:
                    ccxt_metrics["start_time"] = start
                ccxt_metrics["end_time"] = start
                
                # Simulate processing
                orderbook = data.get("data", {})
                _ = len(orderbook.get("bids", [])) + len(orderbook.get("asks", []))
                
                ccxt_metrics["processing_times"].append(time.time() - start)
        
        await connection_manager.subscribe_to_orderbook(
            connection_id, symbol, ccxt_callback, limit=1000
        )
        await asyncio.sleep(test_duration)
        await connection_manager.unsubscribe_from_orderbook(connection_id, symbol)
        await connection_manager.disconnect()
        
        # Analyze results
        print("\n=== Performance Comparison Results ===")
        
        # DepthCacheManager stats
        dc_duration = dc_metrics["end_time"] - dc_metrics["start_time"] if dc_metrics["start_time"] > 0 else test_duration
        dc_update_rate = dc_metrics["updates"] / dc_duration if dc_duration > 0 else 0
        dc_bandwidth_rate = dc_metrics["bandwidth"] / dc_duration / 1024 if dc_duration > 0 else 0
        dc_avg_processing = statistics.mean(dc_metrics["processing_times"]) if dc_metrics["processing_times"] else 0
        
        print("\nDepthCacheManager:")
        print(f"  - Total updates: {dc_metrics['updates']}")
        print(f"  - Update rate: {dc_update_rate:.2f} updates/second")
        print(f"  - Bandwidth: {dc_bandwidth_rate:.2f} KB/s")
        print(f"  - Avg processing time: {dc_avg_processing*1000:.2f} ms")
        
        # CCXT stats
        ccxt_duration = ccxt_metrics["end_time"] - ccxt_metrics["start_time"] if ccxt_metrics["start_time"] > 0 else test_duration
        ccxt_update_rate = ccxt_metrics["updates"] / ccxt_duration if ccxt_duration > 0 else 0
        ccxt_bandwidth_rate = ccxt_metrics["bandwidth"] / ccxt_duration / 1024 if ccxt_duration > 0 else 0
        ccxt_avg_processing = statistics.mean(ccxt_metrics["processing_times"]) if ccxt_metrics["processing_times"] else 0
        
        print("\nCCXT Implementation:")
        print(f"  - Total updates: {ccxt_metrics['updates']}")
        print(f"  - Update rate: {ccxt_update_rate:.2f} updates/second")
        print(f"  - Bandwidth: {ccxt_bandwidth_rate:.2f} KB/s")
        print(f"  - Avg processing time: {ccxt_avg_processing*1000:.2f} ms")
        
        # Comparison
        print("\nComparison:")
        if dc_update_rate > 0 and ccxt_update_rate > 0:
            update_improvement = ((dc_update_rate - ccxt_update_rate) / ccxt_update_rate) * 100
            print(f"  - Update rate difference: {update_improvement:+.1f}%")
        
        if dc_bandwidth_rate > 0 and ccxt_bandwidth_rate > 0:
            bandwidth_change = ((dc_bandwidth_rate - ccxt_bandwidth_rate) / ccxt_bandwidth_rate) * 100
            print(f"  - Bandwidth difference: {bandwidth_change:+.1f}%")
        
        if dc_avg_processing > 0 and ccxt_avg_processing > 0:
            processing_improvement = ((ccxt_avg_processing - dc_avg_processing) / ccxt_avg_processing) * 100
            print(f"  - Processing time improvement: {processing_improvement:+.1f}%")
        
        # Verify both implementations work
        assert dc_metrics["updates"] > 0, "DepthCacheManager received no updates"
        assert ccxt_metrics["updates"] > 0, "CCXT received no updates"
        
        print("\n‚úÖ Performance comparison test completed!")
        
    finally:
        await depth_cache_service.shutdown()


async def test_bandwidth_usage_reduction():
    """Measure bandwidth usage reduction with DepthCacheManager."""
    symbols = ["BTCUSDT", "ETHUSDT"]
    test_duration = 20
    
    dc_bandwidth = {symbol: 0 for symbol in symbols}
    ccxt_bandwidth = {symbol: 0 for symbol in symbols}
    
    async def dc_callback(symbol: str, data: Dict[str, Any]):
        """Track DepthCacheManager bandwidth."""
        dc_bandwidth[symbol] += len(json.dumps(data))
    
    try:
        # Measure DepthCacheManager bandwidth
        logger.info("Measuring DepthCacheManager bandwidth usage...")
        await depth_cache_service.initialize()
        
        for symbol in symbols:
            await depth_cache_service.start_depth_cache(symbol, dc_callback)
            await asyncio.sleep(0.5)
        
        await asyncio.sleep(test_duration)
        
        for symbol in symbols:
            await depth_cache_service.stop_depth_cache(symbol)
        
        # Measure CCXT bandwidth
        logger.info("Measuring CCXT bandwidth usage...")
        connection_manager = ConnectionManager()
        await connection_manager.connect()
        
        connections = {}
        for symbol in symbols:
            connection_id = f"bandwidth_test_{symbol}"
            connections[symbol] = connection_id
            
            async def make_ccxt_callback(sym):
                async def ccxt_callback(data: Dict[str, Any]):
                    if data.get("type") == "orderbook":
                        ccxt_bandwidth[sym] += len(json.dumps(data))
                return ccxt_callback
            
            await connection_manager.subscribe_to_orderbook(
                connection_id, symbol, await make_ccxt_callback(symbol), limit=1000
            )
            await asyncio.sleep(0.5)
        
        await asyncio.sleep(test_duration)
        
        for symbol, connection_id in connections.items():
            await connection_manager.unsubscribe_from_orderbook(connection_id, symbol)
        
        await connection_manager.disconnect()
        
        # Analyze results
        print("\n=== Bandwidth Usage Comparison ===")
        
        total_dc_bandwidth = 0
        total_ccxt_bandwidth = 0
        
        for symbol in symbols:
            dc_kb = dc_bandwidth[symbol] / 1024
            ccxt_kb = ccxt_bandwidth[symbol] / 1024
            
            total_dc_bandwidth += dc_bandwidth[symbol]
            total_ccxt_bandwidth += ccxt_bandwidth[symbol]
            
            reduction_pct = ((ccxt_kb - dc_kb) / ccxt_kb * 100) if ccxt_kb > 0 else 0
            
            print(f"\n{symbol}:")
            print(f"  - DepthCacheManager: {dc_kb:.2f} KB")
            print(f"  - CCXT: {ccxt_kb:.2f} KB")
            print(f"  - Reduction: {reduction_pct:.1f}%")
        
        # Overall summary
        total_dc_kb = total_dc_bandwidth / 1024
        total_ccxt_kb = total_ccxt_bandwidth / 1024
        total_reduction = ((total_ccxt_kb - total_dc_kb) / total_ccxt_kb * 100) if total_ccxt_kb > 0 else 0
        
        print(f"\nTotal bandwidth usage:")
        print(f"  - DepthCacheManager: {total_dc_kb:.2f} KB")
        print(f"  - CCXT: {total_ccxt_kb:.2f} KB")
        print(f"  - Overall reduction: {total_reduction:.1f}%")
        
        # DepthCacheManager should use less bandwidth due to diff updates
        assert total_dc_bandwidth < total_ccxt_bandwidth * 1.2, "DepthCacheManager not showing bandwidth efficiency"
        
        print("\n‚úÖ Bandwidth usage test completed!")
        
    finally:
        await depth_cache_service.shutdown()


async def test_server_side_aggregation():
    """Test server-side aggregation with DepthCacheManager data."""
    symbol = "BTCUSDT"
    rounding_values = [0.1, 1.0, 10.0, 100.0]
    
    try:
        # Initialize service
        await depth_cache_service.initialize()
        
        # Start depth cache
        updates_received = []
        
        async def callback(sym: str, data: Dict[str, Any]):
            updates_received.append(data)
        
        await depth_cache_service.start_depth_cache(symbol, callback)
        
        # Wait for data
        logger.info("Waiting for orderbook data...")
        max_wait = 10
        start_time = time.time()
        
        while len(updates_received) < 5 and (time.time() - start_time) < max_wait:
            await asyncio.sleep(0.5)
        
        assert len(updates_received) > 0, "No updates received"
        
        # Test aggregation with different rounding values
        print("\n=== Server-Side Aggregation Test ===")
        
        for rounding in rounding_values:
            aggregated = await depth_cache_service.aggregate_orderbook(
                symbol, rounding, limit=20
            )
            
            assert aggregated is not None, f"Failed to aggregate with rounding {rounding}"
            assert aggregated["aggregated"] is True
            assert aggregated["rounding"] == rounding
            assert len(aggregated["bids"]) > 0
            assert len(aggregated["asks"]) > 0
            
            # Verify aggregation logic
            for i, (price, amount) in enumerate(aggregated["bids"]):
                # Bids should be sorted descending
                if i > 0:
                    assert price < aggregated["bids"][i-1][0]
                
                # Price should be rounded correctly
                assert price % rounding == 0 or abs(price % rounding) < 0.0001
                
            for i, (price, amount) in enumerate(aggregated["asks"]):
                # Asks should be sorted ascending
                if i > 0:
                    assert price > aggregated["asks"][i-1][0]
                    
                # Price should be rounded correctly
                assert price % rounding == 0 or abs(price % rounding - rounding) < 0.0001
            
            print(f"\nRounding: ${rounding}")
            print(f"  - Bid levels: {len(aggregated['bids'])}")
            print(f"  - Ask levels: {len(aggregated['asks'])}")
            print(f"  - Best bid: ${aggregated['bids'][0][0]:.2f}")
            print(f"  - Best ask: ${aggregated['asks'][0][0]:.2f}")
            print(f"  - Spread: ${aggregated['asks'][0][0] - aggregated['bids'][0][0]:.2f}")
        
        print("\n‚úÖ Server-side aggregation test passed!")
        
    finally:
        # Cleanup
        await depth_cache_service.stop_depth_cache(symbol)
        await depth_cache_service.shutdown()


async def main():
    """Run all tests."""
    print("üß™ OrderFox DepthCacheManager Comprehensive Test Suite")
    print("=" * 80)
    
    # Configure logging
    logging.getLogger('binance').setLevel(logging.WARNING)
    
    # Run tests
    tests = [
        ("Multiple Concurrent Symbols", test_multiple_concurrent_symbols),
        ("Order Book Consistency", test_orderbook_consistency),
        ("Automatic Reconnection", test_automatic_reconnection),
        ("Performance Comparison", test_performance_comparison),
        ("Bandwidth Usage Reduction", test_bandwidth_usage_reduction),
        ("Server-Side Aggregation", test_server_side_aggregation)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n\n{'='*80}")
        print(f"Running: {test_name}")
        try:
            await test_func()
            results.append((test_name, True, None))
        except Exception as e:
            logger.error(f"Test failed with exception: {e}", exc_info=True)
            results.append((test_name, False, str(e)))
        
        # Small delay between tests
        await asyncio.sleep(2)
    
    # Summary
    print("\n\n" + "="*80)
    print("üèÅ Test Results Summary:")
    print("-"*80)
    
    for test_name, success, error in results:
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"{test_name}: {status}")
        if error:
            print(f"  Error: {error}")
    
    passed = sum(1 for _, success, _ in results if success)
    total = len(results)
    print(f"\nüéØ Overall: {passed}/{total} tests passed")
    
    # Recommendations
    if passed == total:
        print("\n\nüìã Phase 1 Testing Complete - All Tests Passed!")
        print("="*80)
        print("‚úÖ DepthCacheManager is working correctly with:")
        print("  - Multiple concurrent symbols")
        print("  - Consistent orderbook data")
        print("  - Automatic reconnection on failures")
        print("  - Better performance than CCXT")
        print("  - Reduced bandwidth usage")
        print("  - Server-side aggregation capability")
        print("\nüöÄ Ready to proceed to Phase 2: Server-Side Order Book Aggregation")


if __name__ == "__main__":
    asyncio.run(main())